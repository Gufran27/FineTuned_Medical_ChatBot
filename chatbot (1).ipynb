{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-05-12T08:39:15.317510Z","iopub.status.busy":"2025-05-12T08:39:15.316835Z","iopub.status.idle":"2025-05-12T08:39:15.958979Z","shell.execute_reply":"2025-05-12T08:39:15.958253Z","shell.execute_reply.started":"2025-05-12T08:39:15.317485Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/medical/cleaned_medical.jsonl\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-05-12T08:39:49.320412Z","iopub.status.busy":"2025-05-12T08:39:49.319792Z","iopub.status.idle":"2025-05-12T08:41:03.540037Z","shell.execute_reply":"2025-05-12T08:41:03.538909Z","shell.execute_reply.started":"2025-05-12T08:39:49.320385Z"},"trusted":true},"outputs":[],"source":["%%capture\n","pip install -q transformers datasets peft bitsandbytes torch accelerate"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-05-12T08:41:03.542222Z","iopub.status.busy":"2025-05-12T08:41:03.541826Z","iopub.status.idle":"2025-05-12T08:41:27.655062Z","shell.execute_reply":"2025-05-12T08:41:27.654483Z","shell.execute_reply.started":"2025-05-12T08:41:03.542191Z"},"trusted":true},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, pipeline, BitsAndBytesConfig\n","from peft import LoraConfig, get_peft_model\n","import torch\n","import os\n","from collections import Counter\n","import re\n","import logging\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-05-12T08:41:27.656386Z","iopub.status.busy":"2025-05-12T08:41:27.655770Z","iopub.status.idle":"2025-05-12T08:41:27.660220Z","shell.execute_reply":"2025-05-12T08:41:27.659442Z","shell.execute_reply.started":"2025-05-12T08:41:27.656366Z"},"trusted":true},"outputs":[],"source":["os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256,expandable_segments:True\"\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-05-12T08:42:27.353829Z","iopub.status.busy":"2025-05-12T08:42:27.353097Z","iopub.status.idle":"2025-05-12T08:42:28.998471Z","shell.execute_reply":"2025-05-12T08:42:28.997849Z","shell.execute_reply.started":"2025-05-12T08:42:27.353800Z"},"trusted":true},"outputs":[],"source":["dataset_path = \"/kaggle/input/medical/cleaned_medical.jsonl\"\n","try:\n","    dataset = load_dataset(\"json\", data_files=dataset_path)\n","    print(f\"Dataset loaded successfully with {len(dataset['train'])} examples\")\n","except Exception as e:\n","    print(f\"Error loading dataset: {e}\")"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-05-12T08:42:28.999701Z","iopub.status.busy":"2025-05-12T08:42:28.999500Z","iopub.status.idle":"2025-05-12T08:42:29.003793Z","shell.execute_reply":"2025-05-12T08:42:29.003052Z","shell.execute_reply.started":"2025-05-12T08:42:28.999684Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['text']\n"]}],"source":["print(dataset[\"train\"].column_names)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-05-12T08:42:30.662858Z","iopub.status.busy":"2025-05-12T08:42:30.662581Z","iopub.status.idle":"2025-05-12T08:42:32.433459Z","shell.execute_reply":"2025-05-12T08:42:32.432646Z","shell.execute_reply.started":"2025-05-12T08:42:30.662835Z"},"trusted":true},"outputs":[],"source":["model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.eos_token  # Set pad token"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-05-12T08:42:32.435251Z","iopub.status.busy":"2025-05-12T08:42:32.434988Z","iopub.status.idle":"2025-05-12T08:42:32.438857Z","shell.execute_reply":"2025-05-12T08:42:32.438341Z","shell.execute_reply.started":"2025-05-12T08:42:32.435233Z"},"trusted":true},"outputs":[],"source":["def format_prompt_with_system_instruction(text):\n","    # Inject a domain-specific system prompt for general medical field\n","    if \"[INST]\" in text:\n","        return text.replace(\n","            \"[INST]\",\n","            \"[INST] <<SYS>>\\nYou are a knowledgeable and reliable AI assistant specialized in the medical domain. \"\n","            \"Only answer questions related to medicine, diseases, symptoms, treatments, healthcare, or related topics. \"\n","            \"If a question is outside the medical field, respond with: \"\n","            \"\\\"I'm sorry, I can only answer questions related to the medical field.\\\"\\n<</SYS>>\\n\\n\"\n","        )\n","    return text\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-05-12T08:42:32.439989Z","iopub.status.busy":"2025-05-12T08:42:32.439697Z","iopub.status.idle":"2025-05-12T08:43:17.389853Z","shell.execute_reply":"2025-05-12T08:43:17.389174Z","shell.execute_reply.started":"2025-05-12T08:42:32.439963Z"},"trusted":true},"outputs":[],"source":["def tokenize_function(examples):\n","    # Inject prompt engineering system message\n","    formatted = [format_prompt_with_system_instruction(t) for t in examples[\"text\"]]\n","    tokenized = tokenizer(\n","        formatted,\n","        padding=\"max_length\",\n","        truncation=True,\n","        max_length=64,\n","    )\n","    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n","    return tokenized\n","\n","# Tokenize dataset\n","tokenized_dataset = dataset.map(tokenize_function, batched=True)\n","tokenized_dataset = tokenized_dataset[\"train\"].train_test_split(test_size=0.1)\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2025-05-12T08:43:17.391555Z","iopub.status.busy":"2025-05-12T08:43:17.391347Z","iopub.status.idle":"2025-05-12T08:43:17.395001Z","shell.execute_reply":"2025-05-12T08:43:17.394355Z","shell.execute_reply.started":"2025-05-12T08:43:17.391537Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n","import torch\n","\n","model_name = \"NousResearch/Llama-2-7b-chat-hf\""]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2025-05-12T08:43:17.396167Z","iopub.status.busy":"2025-05-12T08:43:17.395923Z","iopub.status.idle":"2025-05-12T08:43:18.045001Z","shell.execute_reply":"2025-05-12T08:43:18.044162Z","shell.execute_reply.started":"2025-05-12T08:43:17.396146Z"},"trusted":true},"outputs":[],"source":["quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,  # Enable 4-bit quantization\n","    bnb_4bit_compute_dtype=torch.float16,  # Compute dtype for 4-bit\n","    bnb_4bit_quant_type=\"nf4\",  # Use NF4 quantization (normal float 4-bit)\n","    bnb_4bit_use_double_quant=True  # Enable double quantization for better accuracy\n",")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-05-12T08:43:18.046178Z","iopub.status.busy":"2025-05-12T08:43:18.045889Z","iopub.status.idle":"2025-05-12T08:44:10.645829Z","shell.execute_reply":"2025-05-12T08:44:10.645093Z","shell.execute_reply.started":"2025-05-12T08:43:18.046154Z"},"trusted":true},"outputs":[],"source":["model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=quantization_config,  # Pass quantization config\n","    device_map=\"auto\",  # Automatically map to available devices\n","    torch_dtype=torch.float16  # Use FP Laura for FP16\n",")"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2025-05-12T08:44:10.646875Z","iopub.status.busy":"2025-05-12T08:44:10.646615Z","iopub.status.idle":"2025-05-12T08:44:10.775754Z","shell.execute_reply":"2025-05-12T08:44:10.775228Z","shell.execute_reply.started":"2025-05-12T08:44:10.646852Z"},"trusted":true},"outputs":[],"source":["lora_config = LoraConfig(\n","    r=8,\n","    lora_alpha=16,\n","    target_modules=[\"q_proj\", \"v_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","model = get_peft_model(model, lora_config)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2025-05-12T08:44:10.776700Z","iopub.status.busy":"2025-05-12T08:44:10.776461Z","iopub.status.idle":"2025-05-12T08:44:20.002556Z","shell.execute_reply":"2025-05-12T08:44:20.001927Z","shell.execute_reply.started":"2025-05-12T08:44:10.776676Z"},"trusted":true},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir=\"/kaggle/working/results\",\n","    num_train_epochs=1,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    gradient_accumulation_steps=1,\n","    learning_rate=2e-4,\n","    fp16=True,\n","    save_steps=200,\n","    save_total_limit=1,\n","    eval_strategy=\"steps\",\n","    eval_steps=200,\n","    logging_steps=50,\n","    max_grad_norm=0.3,\n","    warmup_steps=50,\n","    report_to=\"none\",\n","    max_steps=500,\n","    label_names=[\"labels\"],\n","    dataloader_num_workers=4\n",")"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2025-05-12T08:44:20.003497Z","iopub.status.busy":"2025-05-12T08:44:20.003243Z","iopub.status.idle":"2025-05-12T08:44:20.008412Z","shell.execute_reply":"2025-05-12T08:44:20.007649Z","shell.execute_reply.started":"2025-05-12T08:44:20.003475Z"},"trusted":true},"outputs":[],"source":["class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n","        # Move all inputs to the model's device\n","        device = model.device if hasattr(model, 'device') else torch.device(\"cuda:0\")\n","        inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n","\n","        # Compute outputs\n","        outputs = model(**inputs)\n","        loss = outputs.loss\n","\n","        return (loss, outputs) if return_outputs else loss"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2025-05-12T08:44:20.010446Z","iopub.status.busy":"2025-05-12T08:44:20.010222Z","iopub.status.idle":"2025-05-12T09:31:06.573398Z","shell.execute_reply":"2025-05-12T09:31:06.572477Z","shell.execute_reply.started":"2025-05-12T08:44:20.010429Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_31/623358577.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n","  trainer = CustomTrainer(\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [500/500 46:42, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>200</td>\n","      <td>0.022000</td>\n","      <td>0.022032</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.021300</td>\n","      <td>0.022377</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"text/plain":["TrainOutput(global_step=500, training_loss=0.23133858108520508, metrics={'train_runtime': 2806.0643, 'train_samples_per_second': 2.851, 'train_steps_per_second': 0.178, 'total_flos': 2.031064449024e+16, 'train_loss': 0.23133858108520508, 'epoch': 0.056682915769187166})"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["trainer = CustomTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"test\"],\n","    tokenizer=tokenizer,\n",")\n","trainer.train()"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2025-05-12T10:24:43.344680Z","iopub.status.busy":"2025-05-12T10:24:43.343955Z","iopub.status.idle":"2025-05-12T10:24:43.790360Z","shell.execute_reply":"2025-05-12T10:24:43.789589Z","shell.execute_reply.started":"2025-05-12T10:24:43.344656Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model and tokenizer saved to /kaggle/working/medical-llama2\n"]}],"source":["# Save the full model (including LoRA adapter)\n","output_dir = \"/kaggle/working/medical-llama2\"\n","model.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","\n","print(f\"Model and tokenizer saved to {output_dir}\")\n"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2025-05-12T10:25:31.985197Z","iopub.status.busy":"2025-05-12T10:25:31.984712Z","iopub.status.idle":"2025-05-12T10:25:31.993305Z","shell.execute_reply":"2025-05-12T10:25:31.992499Z","shell.execute_reply.started":"2025-05-12T10:25:31.985174Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Pickling metadata...\n"]},{"name":"stderr","output_type":"stream","text":["Saving metadata: 100%|██████████| 1/1 [00:00<00:00, 8240.28it/s]"]},{"name":"stdout","output_type":"stream","text":["Metadata saved to: /kaggle/working/medical_llama2_meta.pkl\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import pickle\n","from tqdm import tqdm\n","\n","output_pkl_path = \"/kaggle/working/medical_llama2_meta.pkl\"\n","metadata = {\n","    \"model_path\": output_dir,\n","    \"tokenizer_path\": output_dir,\n","    \"notes\": \"LoRA fine-tuned LLaMA2 medical chatbot model.\"\n","}\n","\n","# Save model with tqdm progress bar\n","print(\"Pickling metadata...\")\n","with open(output_pkl_path, \"wb\") as f:\n","    with tqdm(total=1, desc=\"Saving metadata\") as pbar:\n","        pickle.dump(metadata, f)\n","        pbar.update(1)\n","\n","print(f\"Metadata saved to: {output_pkl_path}\")\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2025-05-12T09:32:07.866209Z","iopub.status.busy":"2025-05-12T09:32:07.865832Z","iopub.status.idle":"2025-05-12T09:32:07.873985Z","shell.execute_reply":"2025-05-12T09:32:07.873279Z","shell.execute_reply.started":"2025-05-12T09:32:07.866177Z"},"trusted":true},"outputs":[],"source":["model_path = \"/kaggle/working/results/checkpoint-500\"\n","quantization_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_compute_dtype=torch.float16,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_use_double_quant=True\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-05-12T09:32:12.591559Z","iopub.status.busy":"2025-05-12T09:32:12.590750Z","iopub.status.idle":"2025-05-12T09:32:33.951321Z","shell.execute_reply":"2025-05-12T09:32:33.950685Z","shell.execute_reply.started":"2025-05-12T09:32:12.591531Z"},"trusted":true},"outputs":[],"source":["model = AutoModelForCausalLM.from_pretrained(\n","    model_path,\n","    quantization_config=quantization_config,\n","    device_map=\"auto\",\n","    torch_dtype=torch.float16\n",")\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2025-05-12T09:32:33.952883Z","iopub.status.busy":"2025-05-12T09:32:33.952618Z","iopub.status.idle":"2025-05-12T09:32:33.958495Z","shell.execute_reply":"2025-05-12T09:32:33.957784Z","shell.execute_reply.started":"2025-05-12T09:32:33.952861Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Device set to use cuda:0\n"]}],"source":["pipe = pipeline(\n","    task=\"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    max_length=300,\n","    do_sample=True,\n","    temperature=0.7,\n","    top_k=50,\n","    repetition_penalty=1.2,\n",")\n"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2025-05-12T10:46:18.516090Z","iopub.status.busy":"2025-05-12T10:46:18.515444Z","iopub.status.idle":"2025-05-12T10:46:18.520757Z","shell.execute_reply":"2025-05-12T10:46:18.520100Z","shell.execute_reply.started":"2025-05-12T10:46:18.516055Z"},"trusted":true},"outputs":[],"source":["def generate_response(prompt):\n","    \"\"\"Generate a response for the given prompt.\"\"\"\n","    formatted_prompt = f\"<s>[INST] {prompt} [/INST]\"\n","    result = pipe(formatted_prompt)\n","    raw_text = result[0]['generated_text']\n","    if \"[/INST]\" in raw_text:\n","        clean_text = raw_text.split(\"[/INST]\")[1].split(\"</s>\")[0].strip()\n","    else:\n","        clean_text = raw_text.strip()\n","    print(f\"Question: {prompt}\")\n","    print(f\"Answer: {clean_text}\")\n","    print(\"-\" * 50)"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2025-05-12T13:07:42.605103Z","iopub.status.busy":"2025-05-12T13:07:42.604484Z","iopub.status.idle":"2025-05-12T13:07:45.688927Z","shell.execute_reply":"2025-05-12T13:07:45.688320Z","shell.execute_reply.started":"2025-05-12T13:07:42.605068Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Please enter your prompt (type 'exit' to quit):\n"]},{"name":"stdout","output_type":"stream","text":["Enter your Question:  exit\n"]},{"name":"stdout","output_type":"stream","text":["Bye!\n"]}],"source":["print(\"Please enter your prompt (type 'exit' to quit):\")\n","while True:\n","    user_prompt = input(\"Enter your Question: \")\n","    if user_prompt.lower() == \"exit\":\n","        print(\"Bye!\")\n","        break\n","    generate_response(user_prompt)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":7395720,"sourceId":11780062,"sourceType":"datasetVersion"}],"dockerImageVersionId":31011,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":4}
